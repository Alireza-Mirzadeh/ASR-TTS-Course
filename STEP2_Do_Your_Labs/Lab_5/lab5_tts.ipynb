{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Text-to-Speech (TTS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Begin\n",
    "\n",
    "This is a practical part of your ASR-TTS course. In total you will have 5 labs. Three of which will be focused on Automatic Speech Recognition and two on Text-to-Speech models. Each lab will last two hours and consist of two parts:\n",
    "* Reading Part\n",
    "* Coding Part \n",
    "\n",
    "In each part you might find question or tasks/activities to complete. \n",
    "\n",
    "LAB 5/5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will you learn in LAB 5?\n",
    "* Review and reinforce the core building blocks of ASR and TTS systems\n",
    "* Understand how speaker identity is encoded in speech signals\n",
    "* Distinguish between what is said (linguistic content) and who says it (speaker identity)\n",
    "* Understand why speaker identity can be considered sensitive personal data\n",
    "* Learn the motivation behind speaker anonymization and voice privacy protection\n",
    "* Analyze how modern speech systems can unintentionally leak speaker identity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions: ASR Revision**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Give two advantages and two limitations and explain the difference in the architecture of:\n",
    "* traditional modular ASR systems\n",
    "* end-to-end ASR systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Explain why feature extraction is needed in ASR and outline the main steps involved, from waveform to feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Explain how speech conversion works.\n",
    "Help: https://arxiv.org/abs/2008.03648\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions: TTS Revision**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the architecture of a TTS model and what each part is responsible for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What is a triphone. Name one advantage and one disadvantage of using it.\n",
    "\n",
    "https://www.researchgate.net/publication/4194601_Biphone-rich_versus_triphone-rich_a_comparison_of_speech_corpora_in_automatic_speech_recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How can we evaluate a TTS model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaker identification is the task of recognizing who is speaking from a voice recording. While ASR focuses on understanding what is being said and TTS focuses on producing speech, speaker identification focuses on the person behind the voice. This is important because modern speech systems can automatically learn and recognize speaker identity, even when this is not their main goal. When learning about ASR and TTS, it is therefore important to understand that speech technology does not only process words, but also personal information such as voice characteristics. This is closely related to speaker privacy and anonymization, which aim to protect speakers from being identified. The SpeechBrain toolkit includes models not only for ASR and TTS, but also for speaker identification and speaker embedding extraction, allowing us to study all these aspects within the same framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use a pre-trained speaker embedding model from SpeechBrain to extract speaker representations from audio and identify who is speaking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Questions: Speaker Identification**</span>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think and explain what kind of systems could use speaker identification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the following research paper (pages 442-444) and explain what information about the person can be extracted from his voice and why is it dangerous.\n",
    "\n",
    "https://encrypto.de/papers/NJTKHMAATMGPCESBRTB19.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.4.1+cu121\n",
      "torchaudio: 2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch, torchaudio\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchaudio:\", torchaudio.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will need to install pyannote library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"pyannote.audio==3.4.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/speechbrain/speechbrain.git@develop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "from pyannote.audio import Pipeline\n",
    "from scipy.spatial.distance import cdist\n",
    "from huggingface_hub import hf_hub_download\n",
    "import numpy as np\n",
    "import soundfile as sf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for us to have a conversation where different speakers can be identified, we will first join three separate audio recordings into a single audio file, adding a 1-second pause between each recording. This will simulate a simple conversation where different people speak one after another. We will then use a pre-trained speaker model to analyze the combined audio and automatically determine who is speaking and when. Through this process, you will see how speaker identity can be detected from speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input files of three different speakers from your labs dataset\n",
    "wav_files = [\n",
    "\n",
    "]\n",
    "\n",
    "# Pause duration (seconds)\n",
    "pause_duration = 1.0  # 1 second pause\n",
    "\n",
    "# Read first file to get sample rate\n",
    "audio_list = []\n",
    "data, sr = sf.read(wav_files[0])\n",
    "audio_list.append(data)\n",
    "\n",
    "# Create silence\n",
    "pause = np.zeros(int(pause_duration * sr))\n",
    "\n",
    "# Process remaining files\n",
    "for wav in wav_files[1:]:\n",
    "    audio_list.append(pause)\n",
    "    data, sr_i = sf.read(wav)\n",
    "    assert sr_i == sr, \"Sample rates must match\"\n",
    "    audio_list.append(data)\n",
    "\n",
    "# Concatenate everything\n",
    "final_audio = np.concatenate(audio_list)\n",
    "\n",
    "# Save output\n",
    "sf.write(\"combined.wav\", final_audio, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT NECESSARY TO RUN IF YOU DO NOT HAVE A GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model for speaker embedding extraction and move it to the device\n",
    "# Note: You need to obtain an API key from Hugging Face to use this model.\n",
    "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", run_opts={\"device\": device})\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "\n",
    "For this part of the lab, you will need to register on Hugging Face, because some pre-trained speech models require an access token. This is mainly due to licensing, usage conditions, and privacy considerations, especially for models that process or reveal speaker identity. After creating a free Hugging Face account, you must accept the usage conditions of the speaker diarization model and generate a read-only access token, which allows your code to download and use the model. This token does not cost anything and is only used to verify that you have agreed to the model’s conditions.\n",
    "\n",
    "Here is what you should do:\n",
    "\n",
    "* Register and login: https://huggingface.co\n",
    "* Accept the conditions of the following three:\n",
    "* pyannote/speaker-diarization-3.1\n",
    "* pyannote/segmentation-3.0\n",
    "* pyannote/wespeaker-voxceleb-resnet34-LM\n",
    "* Go to settings -> access tokens -> read token\n",
    "* Mark all fields, create and copy/paste the token below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Important security warning:**</span>\n",
    "Never push access tokens to GitHub or any public repository.\n",
    "Before submitting or sharing your lab work, you must delete your token from the notebook or script. Tokens should be treated like passwords. If you accidentally share a token, you should delete it immediately from your Hugging Face account and create a new one. In addition, do not upload you token into public AI chatbots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model for speaker diarization\n",
    "# Note: The speaker diarization model also requires an API key from Hugging Face.\n",
    "diarization = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n",
    "                                        use_auth_token=\"YOUR_HUGGING_FACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This speaker diarization system works by listening to the audio and automatically detecting when speech is present and when it changes from one speaker to another. For each short speech segment, the model extracts characteristics that are typical of a person’s voice. These characteristics are summarized into a compact numerical representation. Segments with similar voice characteristics are then grouped together, so that parts spoken by the same person are assigned the same speaker label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known matrix shape: (3, 192)\n"
     ]
    }
   ],
   "source": [
    "known_speaker_files = [\n",
    "    (\"path1\", \"Steve Jobs\"),\n",
    "    (\"path2\", \"Elon Musk\"),\n",
    "    (\"path3\", \"Nelson Mandela\"),\n",
    "]\n",
    "\n",
    "known_speakers = []\n",
    "known_speaker_ids = []\n",
    "\n",
    "for wav_path, name in known_speaker_files:\n",
    "    wav, sr = torchaudio.load(wav_path) \n",
    "    if wav.shape[0] > 1:\n",
    "        wav = wav.mean(dim=0, keepdim=True)\n",
    "    wav = wav.to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = classifier.encode_batch(wav)  \n",
    "    emb_vec = emb.squeeze().detach().cpu().numpy().reshape(-1)  \n",
    "    known_speakers.append(emb_vec)\n",
    "    known_speaker_ids.append(name)\n",
    "\n",
    "known_matrix = np.vstack(known_speakers)  \n",
    "\n",
    "print(\"Known matrix shape:\", known_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"/home/aine/Teaching/Lab_5/combined.wav\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = diarization(audio_path, min_speakers=1, max_speakers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = torchaudio.info(audio_path)\n",
    "sr = info.sample_rate\n",
    "\n",
    "threshold = 0.8  # tune\n",
    "\n",
    "for segment, track, label in segments.itertracks(yield_label=True):\n",
    "    start_time, end_time = segment.start, segment.end\n",
    "\n",
    "    frame_offset = int(start_time * sr)\n",
    "    num_frames = int((end_time - start_time) * sr)\n",
    "    if num_frames <= 0:\n",
    "        continue\n",
    "\n",
    "    wav_seg, _ = torchaudio.load(audio_path, frame_offset=frame_offset, num_frames=num_frames)\n",
    "\n",
    "    if wav_seg.shape[0] > 1:\n",
    "        wav_seg = wav_seg.mean(dim=0, keepdim=True)\n",
    "\n",
    "    wav_seg = wav_seg.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb = classifier.encode_batch(wav_seg)\n",
    "\n",
    "    emb_query = emb.squeeze().detach().cpu().numpy().reshape(1, -1)  \n",
    "    # cosine distance to all known speakers\n",
    "    distances = cdist(emb_query, known_matrix, metric=\"cosine\").flatten()  \n",
    "    best_i = int(np.argmin(distances))\n",
    "    best_dist = float(distances[best_i])\n",
    "    best_name = known_speaker_ids[best_i]\n",
    "\n",
    "    if best_dist < threshold:\n",
    "        print(f\"{best_name} ({label}) : {start_time:.2f}s–{end_time:.2f}s  dist={best_dist:.3f}\")\n",
    "    else:\n",
    "        print(f\"Unknown ({label}) : {start_time:.2f}s–{end_time:.2f}s  best={best_name} dist={best_dist:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Question: exercise 1**</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get into groups of three and record a conversation together plus one separate audio for each of your voices and run the speaker identification model. Discuss, how accurate is the model. Did it identify you correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Anonymization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\"> **Question: Anonymization Techniques**</span>\n",
    "\n",
    "Read the following research paper and describe two possible anonymization techniques:\n",
    "https://inria.hal.science/hal-04667625/file/panariello_TASLP24.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this final lab session was to highlight that working with speech and voice technologies is not only a technical task, but also one that carries significant ethical and societal risks. Modern TTS and voice processing systems make it possible to manipulate speaker identity, generate realistic synthetic voices, and alter speech in ways that can be misused if not handled responsibly. By examining speaker identification and anonymization, this lab aimed to show that voice is a biometric signal and that its manipulation should not be underestimated. Understanding these risks is an important part of training for anyone who works with speech technologies.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
