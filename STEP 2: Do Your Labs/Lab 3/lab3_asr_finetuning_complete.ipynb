{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a Wav2vec Model for Automatic Speech Recognition\n",
    "\n",
    "**Name**: \n",
    "\n",
    "The ASR system used in the first two labs relied on an acoustic model made up of two main components: a *feature* extractor (based on the [wav2vec](https://arxiv.org/pdf/2006.11477) architecture) and a *classifier* (a linear layer). The model then operates in two stages:\n",
    "\n",
    "1. The speech waveform is passed to the feature extractor to transform it into a latent representation (= the *feature maps*).\n",
    "2. The latent representation is passed to a classification layer to compute the probability of each character (= the *emission matrix*).\n",
    "\n",
    "<center><img src=\"https://github.com/magronp/magronp.github.io/blob/master/images/wav2vec2asr.png?raw=true\" width=\"800\"></center>\n",
    "\n",
    "To train such a model in practice, a two-stage proces is used. First, the wav2vec model alone is pre-trained in a self-supervised manner (using speech data only). Then, the classification layer is added and the whole model \\{wav2vec+classifier\\} is fine-tuned in a supervised manner (from speech+transcript data).\n",
    "\n",
    "In the previous labs, we have used a whole model that was already pre-trained and fine-tuned for ASR. In this lab, we start from the pre-trained wav2vec model, and we reproduce the process of fine-tuning it for ASR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "torch.random.manual_seed(0);\n",
    "\n",
    "MAX_FILES = 100 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech and transcripts sub-directories paths\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example file\n",
    "audio_file = '61-70968-0001.wav'\n",
    "audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "print(f\"Audio file path: {audio_file_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a transcript; the preprocessing here is a bit different than in previous labs (more convenient for training)\n",
    "def get_true_transcript(transc_file_path):\n",
    "    with open(transc_file_path, \"r\") as f:\n",
    "        true_transcript = f.read()\n",
    "    true_transcript = true_transcript.lower().replace(' ','|').replace('\\n','')\n",
    "    return true_transcript\n",
    "\n",
    "# Load and display the true transcription\n",
    "transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "true_transcript = get_true_transcript(transc_file_path)\n",
    "print(true_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the list of labels (=characters) that can be found in the dataset\n",
    "labels = ['-', '|', 'e', 't', 'a', 'o', 'n', 'i', 'h', 's', 'r', 'd', 'l', 'u', 'm',\n",
    "          'w', 'c', 'f', 'g', 'y', 'p', 'b', 'v', 'k', \"'\", 'x', 'j', 'q', 'z']\n",
    "n_labels = len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the true transcript into a list of integers in order to feed it to a training loss function. To that end, we define a dictionary `dico_labels` that maps each character in the list of possible labels to an integer (for instance, `dico_labels['e']=2` or `dico_labels['a']=4`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: using the 'labels' list above, define this dictionary\n",
    "dico_labels = {}\n",
    "for index, element in enumerate(labels):\n",
    "    dico_labels[element] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dico_labels to the true transcript, and build a tensor from it\n",
    "target_indices = [dico_labels[c] for c in true_transcript]\n",
    "target_indices = torch.tensor(target_indices, dtype=torch.long)\n",
    "print(target_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoustic model\n",
    "\n",
    "Torchaudio comprises many models whose pretrained weights can be loaded directly (the list can be found [here](https://pytorch.org/audio/stable/pipelines.html#id3)). Here we use `WAV2VEC2_BASE`, which is pre-trained on speech data, but not fine-tuned for ASR.\n",
    "\n",
    "We can apply it to the waveform to compute the feature maps, which is a tensor of size `[1, time steps, feature dim]` (recall that `1` corresponds to the batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the acoustic model: here it's a Wav2vec that is not fine-tuned\n",
    "model_name = 'WAV2VEC2_BASE'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "wav2vec = bundle.get_model()\n",
    "\n",
    "# Display model architecture\n",
    "print(wav2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the feature maps\n",
    "with torch.inference_mode():\n",
    "    features, _ = wav2vec(waveform)\n",
    "\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 1**</span>. Complete the `Wav2vecASR` model class below. This model includes a wav2vec feature extractor (so it should be re-instaciated in the `__init__` method) and a linear classification layer that takes the feature maps and outputs log-probabilities per class (thus it has to use a [log softmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html) activation after classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wav2vecASR(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wav2vec = getattr(torchaudio.pipelines, 'WAV2VEC2_BASE').get_model()\n",
    "        self.feature_dim = self.wav2vec.encoder.feature_projection.projection.out_features\n",
    "        #self.feature_dim = 768 #the easy way\n",
    "        self.output_layer = nn.Linear(self.feature_dim, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features, _ = self.wav2vec(x)\n",
    "        emission = self.output_layer(features)\n",
    "        emission = emission.log_softmax(dim=-1)\n",
    "        return emission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the model\n",
    "output_size = n_labels\n",
    "model = Wav2vecASR(output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to count the number of trainable parameters\n",
    "def count_tlearnable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print('Number of trainable parameters:', count_tlearnable_params(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is very large (90M+ parameters) mostly because of the wav2vec part. To speed training in this lab, we freeze the wav2vec part and we only train the classification layer. To that end, we need to set `requires_grad = False` for the wav2vec's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define a function to freeze parameters, apply it to \"wav2vec\" part of the model,\n",
    "# and print the new number of trainable parameters\n",
    "def freeze_params(m):\n",
    "    for param in m.parameters():\n",
    "        param.requires_grad = False\n",
    "    return\n",
    "    \n",
    "model.wav2vec.apply(freeze_params)\n",
    "\n",
    "print('Number of trainable parameters:', count_tlearnable_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization function for the network's parameters\n",
    "def init_params(m, seed=0):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data, generator=torch.manual_seed(seed))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0.01)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply the initialization function to the classification layer of the model\n",
    "model.output_layer.apply(init_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TODO: apply the model to the waveform to compute the emission matrix, print its shape and plot it\n",
    "with torch.inference_mode():\n",
    "    emission = model(waveform)\n",
    "print(emission.shape)\n",
    "\n",
    "# Vizualize the emission matrix\n",
    "plt.figure()\n",
    "plt.imshow(emission[0].cpu().T)\n",
    "plt.title(\"Emission matrix\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a function to get a transcript from the emission matrix (similar to the greedy decoder from lab 1)\n",
    "def transcript_from_emission(emission, labels):\n",
    "    indices = torch.argmax(emission, dim=-1)  # take the most likely index at each time step\n",
    "    indices = torch.unique_consecutive(indices, dim=-1) # remove duplicates\n",
    "    indices = [i for i in indices if i != 0] # remove the blank token\n",
    "    transcript = \"\".join([labels[i] for i in indices]) # convert integers back into characters\n",
    "    transcript = transcript.lower()\n",
    "    return transcript\n",
    "\n",
    "# The transcript using a non-trained model should look bad\n",
    "print(transcript_from_emission(emission[0], labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We now need to define the `Dataset` class, to efficiently load the speech data and true transcript (in the form of indices / integers).\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 2**</span>. Complete the `ASRdataset` class below (`__init__`, `__len__`, and `__getitem__` methods). In particular, the `__getitem__` method should return three outputs:\n",
    "- `waveform`: a tensor containing the speech waveform\n",
    "- `true_transcript`: the true transcript as per using the provided loading function\n",
    "- `target_indices`: a tensor containing the integers corresponding to the transcript\n",
    "\n",
    "The `__init__` method should make use of the `MAX_FILES` variable to limit the size of the dataset (for speed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASR Dataset class\n",
    "class ASRdataset(Dataset):\n",
    "    def __init__(self, data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=None):\n",
    "        self.data_speech_dir = data_speech_dir\n",
    "        self.data_transc_dir = data_transc_dir\n",
    "        self.audio_files = self._find_files(data_speech_dir)[:MAX_FILES]\n",
    "        self.dico_labels = dico_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # load the waveform\n",
    "        audio_file = self.audio_files[index]\n",
    "        audio_file_path = os.path.join(self.data_speech_dir, audio_file)\n",
    "        waveform, _ = torchaudio.load(audio_file_path, channels_first=True)\n",
    "        # load the true transcript\n",
    "        transc_file_path = os.path.join(self.data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "        true_transcript = get_true_transcript(transc_file_path)\n",
    "        # transform transcript into list of integers\n",
    "        target_indices = [self.dico_labels[c] for c in true_transcript]\n",
    "        target_indices = torch.tensor(target_indices, dtype=torch.long)\n",
    "        return waveform, true_transcript, target_indices\n",
    "\n",
    "    def _find_files(self, directory, pattern='*.wav'):\n",
    "        \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "        files = []\n",
    "        for root, _, filenames in os.walk(directory):\n",
    "            for filename in fnmatch.filter(filenames, pattern):\n",
    "                files.append(filename)\n",
    "        files = sorted(files)\n",
    "        return files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate the ASR dataset and print its length\n",
    "asrdataset = ASRdataset(data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=MAX_FILES)\n",
    "print('Dataset length:', len(asrdataset))\n",
    "\n",
    "# Get the first data sample, and print some information\n",
    "waveform, true_transcript, target_indices = asrdataset[0]\n",
    "print(waveform.shape)\n",
    "print(true_transcript)\n",
    "print(target_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-life applications (and as we usually do in the \"Neural Networks\" labs), we assemble the data samples into *batches* for efficiency. However, our data points here have different lenghts in general: two speech waveforms / two transcripts are not guaranteed to have the same duration / number of characters. Therefore, in such a case we need to customize the dataloader such that it performs some [padding operation](https://www.codefull.org/2018/11/use-pytorchs-dataloader-with-variable-length-sequences-for-lstm-gru/) in order to yield data samples of same length. However, to keep things simple in this lab, we skip this padding operation, and we do not work with batches but rather iterate over the dataset directly.\n",
    "\n",
    "## Training with the CTC loss\n",
    "\n",
    "For a given data sample in the dataset, we have:\n",
    "- the emission matrix (log-probability of each character over time frames)\n",
    "- the true transcript (represented as a list of integers corresponding to each character)\n",
    "\n",
    "However, to train our network, we need to obtain an estimated transcript so that we can compute a loss between the true and estimated transcripts. This requires some post-processing of the emission matrix, but the good news is that we don't have to do it, the [CTC loss](https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html) handles that for us!\n",
    "\n",
    "In lab 2 we have use the CTC algorithm to perform inference, but it can also be used as a loss function to train an ASR network. Not only the CTC loss handles the post-processing from the emission matrix, but its great advantage is that it performs alignment from input/output pairs of different lengths, so we don't have to explicitly align each character in the transcript with a time frame in the emission matrix.\n",
    "\n",
    "In Pytorch, [CTC loss](https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html) is fed with the the emission matrix and the tensor containing target indices (corresponding to the true transcript). We also need to give it the input and target lengths explicitly: indeed, even though here we don't manipulate batches / we don't do padding, in general this would be the case so we need to let the function know what is the actual input/target length (before padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the emission matrix (not in inference mode such that we keep track of the gradients)\n",
    "emission = model(waveform)\n",
    "emission = emission[0] #remove the \"batch\" dimension (since here batch_size=1)\n",
    "\n",
    "# Define the input and target lengths as tensors\n",
    "T = emission.shape[0]\n",
    "L = len(target_indices)\n",
    "input_length = torch.tensor(T, dtype=torch.long)\n",
    "target_length = torch.tensor(L, dtype=torch.long)\n",
    "\n",
    "# Instanciate a loss object\n",
    "ctc_loss = nn.CTCLoss()\n",
    "\n",
    "# Compute the loss\n",
    "loss = ctc_loss(emission, target_indices, input_length, target_length)\n",
    "print(loss.item())\n",
    "\n",
    "# Compute the gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 3**</span>. Write the training function `training_wav2vecASR` to train the model. It is similar to what we usually do in the Neural Networks labs, although here we do not build batches of data (thus we don't need a `Dataloader` object). Instead, we directly iterate over the `Dataset`. The training function uses an Adam optimizer, and no validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function\n",
    "def training_wav2vecASR(model, train_dataloader, num_epochs, loss_fn, learning_rate):\n",
    "\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    model_tr.train()\n",
    "    optimizer = torch.optim.Adam(model_tr.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        tr_loss = 0\n",
    "        for data_sample in asrdataset:\n",
    "            \n",
    "            # Get the data\n",
    "            waveform, true_trans, target_indices = data_sample\n",
    "            \n",
    "            # Apply the model\n",
    "            emission = model_tr(waveform)\n",
    "            emission = emission.squeeze()\n",
    "            \n",
    "            #print(true_trans)\n",
    "            #print(transcript_from_emission(emission, labels))\n",
    "\n",
    "            # Get the input and target lengths\n",
    "            input_length = torch.tensor(emission.shape[0], dtype=torch.long)\n",
    "            target_length = torch.tensor(len(target_indices), dtype=torch.long)\n",
    "    \n",
    "            # Compute the CTC loss\n",
    "            loss = loss_fn(emission, target_indices, input_length, target_length)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            tr_loss += loss.item()\n",
    "    \n",
    "        # Normalize and store loss\n",
    "        tr_loss = tr_loss / len(asrdataset)\n",
    "        train_losses.append(tr_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {tr_loss:.4f}\")\n",
    "\n",
    "    return model_tr, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters (only one epoch for speed)\n",
    "num_epochs = 1\n",
    "learning_rate = 0.1\n",
    "loss_fn = nn.CTCLoss()\n",
    "\n",
    "model_tr, train_losses = training_wav2vecASR(model, asrdataset, num_epochs, loss_fn, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting on one sample\n",
    "\n",
    "We observed above that training is slow: this is because the wav2vec model is rather large so the forward pass takes some time (even though the gradients are not tracked), and also because we did not assemble data into batches.\n",
    "\n",
    "Nonetheless, we can assess that our training pipeline works properly by conducting training on a unique sample, and checking the transcript on the same sample. This technique (= overfitting on one sample) is useful to \"crash test\" if everything runs and if our model / training pipeline has any chance to work on a larger dataset.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 4**</span>. Instanciate a model from scratch, freeze the wav2part, and initialize the classification layer. Build a dataset made up of 1 sample (use the `MAX_FILES` parameter), and conduct training for 50 epoch using this dataset. Once the model is trained, compute the emission matrix from this sample's waveform and the estimated transcript. Display the true and estimated transcripts. Also do the same on another sentence so that you can assess that the model is not able to generalize properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate a model, freeze the wav2vec part, and initialize the classifier\n",
    "model = Wav2vecASR(n_labels)\n",
    "model.wav2vec.apply(freeze_params)\n",
    "model.output_layer.apply(init_params)\n",
    "\n",
    "# Build a very small dataset\n",
    "asrdataset = ASRdataset(data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=1)\n",
    "\n",
    "# Training\n",
    "num_epochs = 50\n",
    "model_tr, train_losses = training_wav2vecASR(model, asrdataset, num_epochs, loss_fn, learning_rate)\n",
    "\n",
    "# Save the model's parameters\n",
    "torch.save(model_tr.state_dict(), 'model_wav2vecASR.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data sample\n",
    "waveform, true_transcript, _ = asrdataset[0]\n",
    "\n",
    "# Instanciate the model and load the trained parameters\n",
    "model = Wav2vecASR(n_labels)\n",
    "model.load_state_dict(torch.load('model_wav2vecASR.pt', weights_only=True))\n",
    "\n",
    "# Apply the model and get the estimated transcript\n",
    "with torch.inference_mode():\n",
    "    emission = model(waveform)\n",
    "    est_transcript = transcript_from_emission(emission[0], labels)\n",
    "\n",
    "# Display the true and estimated transcripts\n",
    "print('True transcript: ', true_transcript)\n",
    "print('Estimated transcript: ', est_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try on a different sentence\n",
    "asrdataset = ASRdataset(data_speech_dir, data_transc_dir, dico_labels, MAX_FILES=2)\n",
    "waveform, true_transcript, _ = asrdataset[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    emission = model(waveform)\n",
    "    est_transcript = transcript_from_emission(emission[0], labels)\n",
    "\n",
    "print('True transcript (other sentence): ', true_transcript)\n",
    "print('Estimated transcript (other sentence): ', est_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
