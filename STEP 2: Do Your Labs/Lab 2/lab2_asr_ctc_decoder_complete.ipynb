{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Speech Recognition with CTC Decoder\n",
    "\n",
    "**Name**:\n",
    "\n",
    "The first lab introduced the building blocks of an ASR system, including feature extraction and classification with an acoustic model (wav2vec2), which produced an *emission* matrix (probability for each character at each time step). From this emission matrix, we could compute the most likely character at each time step using a naïve *greedy* decoder. The drawback of such an approach is the lack of context, which can produce sequences of characters that do not correspond to actual words, and/or sequences of words that are incorrect / do not correspond to any language rules.\n",
    "\n",
    "In this lab, we introduce the usage of a more advanced decoding technique that is based on [connectionist temporal classification](https://towardsdatascience.com/intuitively-understanding-connectionist-temporal-classification-3797e43a86c) (CTC). The general idea of such a decoder is to consider some context (sequences of characters, possible words, and possible sequences of words), in oder to yield more likely / realistic outputs than those given by the greedy decoder.\n",
    "\n",
    "<center><a href=\"https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*XbIp4pn38rxL_DwzSeYVxQ.png\" width=\"400\"></a></center>\n",
    "\n",
    "To do so, the CTC decoder relies on three main components:\n",
    "    \n",
    "- A **beam search**, which is an algorithm to efficently find the *best path* from the emission matrix, that is, the sequence of characters with highest probability (rather than the sequence of individually most likely characters).\n",
    "- A **lexicon**, which is a mapping between token sequences (list of characters) and words. It is used to restrict the search space of the decoder to words that only belong to this dictionary (e.g., the word \"azfpojazflj\" does not exist in the English vocabulary).\n",
    "- A **language model**, which specifies sequences of words that are more likely to occur than others. A common choice of language model is an $n$-gram, which is a statistical model for the probability of occurrence of a given word based on the previous $n$ ones (for instance, the sequence \"the sky is\" is more likely to be followed with \"blue\" rather than \"trumpet\").\n",
    "\n",
    "The CTC decoder combines these ingredients to compute the score of several word sequences (or *hypothesis*), in order to find the best possible transcript. In this lab, we study the influence of the lexicon, language model, and the beam search size onto ASR performance from a practical perspective, without going into the technical details of the [beam search algorithm](https://www.width.ai/post/what-is-beam-search) or the [CTC loss](https://distill.pub/2017/ctc/), which can also be used for training the network (as we will see in lab 3).\n",
    "\n",
    "**Note**: This lab is based on this [tutorial](https://pytorch.org/audio/main/tutorials/asr_inference_with_ctc_decoder_tutorial.html), which you can check for more details on CTC decoder parameters in torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.models.decoder import ctc_decoder, download_pretrained_files\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "torch.random.manual_seed(0)\n",
    "MAX_FILES = 2 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech and transcripts sub-directories paths\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "As in the previous lab, we first load an example speech signal, and we display it. We also provide the function to get the true transcript and compute the WER. Finally, we load the wav2vec2 acoustic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example file\n",
    "audio_file = '61-70968-0001.wav'\n",
    "audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "print(f\"Audio file path: {audio_file_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide the function for loading the true transcript and computing the WER\n",
    "def get_true_transcript(transc_file_path):\n",
    "    with open(transc_file_path, \"r\") as f:\n",
    "        true_transcript = f.read()\n",
    "    true_transcript = true_transcript.lower().split()\n",
    "    return true_transcript\n",
    "\n",
    "def get_wer(true_transcript, est_transcript):\n",
    "    wer = torchaudio.functional.edit_distance(true_transcript, est_transcript) / len(true_transcript)\n",
    "    return wer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display the true transcription\n",
    "transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "true_transcript = get_true_transcript(transc_file_path)\n",
    "print(true_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the acoustic model\n",
    "model_name = 'WAV2VEC2_ASR_BASE_100H'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "acoustic_model = bundle.get_model()\n",
    "labels = bundle.get_labels()\n",
    "\n",
    "# Apply the model to the waveform to get the emission tensor\n",
    "with torch.inference_mode():\n",
    "    emission, _ = acoustic_model(waveform)\n",
    "    num_time_steps = emission.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC Decoder\n",
    "\n",
    "The CTC decoder can be constructed directly by using the `ctc_decoder` function in torchaudio. In addition to the parameters related to the beam search (we will study them later on), it takes as inputs:\n",
    "- the list of tokens, in order to map emissions to characters in the classifier.\n",
    "- the path to the lexicon, expected as a .txt file containing, on each line, a word followed by its space-split tokens (and special end-of-sequence token `|`):\n",
    "\n",
    "```\n",
    "# lexicon.txt\n",
    "a     a |\n",
    "able  a b l e |\n",
    "about a b o u t |\n",
    "...\n",
    "```\n",
    "- the path to the language model, expected as a .bin file.\n",
    "\n",
    "All these are assembled in pretrained files that can be downloaded using the `download_pretrained_files` function (this might take some time as the language model can be large), and then used to contruct the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Download the files corresponding to (pretrained) language model, which comes with lexicon and tokens\n",
    "files = download_pretrained_files(\"librispeech\")\n",
    "path_lm_tokens = files.tokens\n",
    "path_lm_lexicon = files.lexicon\n",
    "path_lm = files.lm\n",
    "\n",
    "print(path_lm_tokens)\n",
    "print(path_lm_lexicon)\n",
    "print(path_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the first 10 tokens (includes the blank and end-of-word token)\n",
    "with open(path_lm_tokens, 'r') as f:\n",
    "    tok = f.read().splitlines()\n",
    "print(\"\\n\".join(tok[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize the lexicon content (first 10 entries)\n",
    "with open(path_lm_lexicon, 'r') as f:\n",
    "    lex = f.read().splitlines()\n",
    "print(\"\\n\".join(lex[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To obtain the line(s) corresponding to a word in the lexicon, you can use the following:\n",
    "w = 'hello'\n",
    "lex_w = [line for line in open(path_lm_lexicon) if line.startswith(w + '\\t')] # it's a list since there could be different pronunciation for the same word\n",
    "print(lex_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate the CTC decoder\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=path_lm_lexicon,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "\n",
    "# Apply the decoder to the `emission` tensor, and get the first element (batch_size=1) and best hypothesis\n",
    "ctc_decoder_result = decoder(emission)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "The decoder output `ctc_decoder_result` contains many fields, including the predicted token IDs, and a `.words` field that contains the transcript as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the token IDs using the .tokens field\n",
    "ctc_decoder_indices = ctc_decoder_result.tokens\n",
    "print(f\"Token indices: {ctc_decoder_indices}\")\n",
    "\n",
    "# Display the transript using the .words field\n",
    "print(f\"Words: {ctc_decoder_result.words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Get the token IDs using the .tokens field\n",
    "ctc_decoder_indices = ctc_decoder_result.tokens\n",
    "print(f\"Token indices: {ctc_decoder_indices}\")\n",
    "\n",
    "# Get the words vis the .words field\n",
    "print(f\"Words: {ctc_decoder_result.words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy decoder\n",
    "\n",
    "The greedy decoder we have seen in the first lab is a particular case of the CTC decoder when no langage model / lexicon is provided. It can be constructed by simply passing `None` as corresponding input arguments in the `ctc_decoder` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: Construct a greedy decoder (no LM / lexicon), and apply it to the emission matrix\n",
    "decoder_nolm = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=None\n",
    ")\n",
    "\n",
    "ctc_nolm_result = decoder_nolm(emission)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since no language model is provided, the .words field returns an empty list\n",
    "print(ctc_nolm_result.words)\n",
    "\n",
    "# Then we have to manually convert token IDs to tokens using the decoder.idxs_to_tokens method\n",
    "# (+ a bit of postprocessing)\n",
    "ctc_nolm_tokens = decoder.idxs_to_tokens(ctc_nolm_result.tokens)\n",
    "ctc_nolm_transcript = \"\".join(ctc_nolm_tokens).replace(\"|\", \" \").strip().split() \n",
    "ctc_nolm_transcript = [w.lower() for w in ctc_nolm_transcript]\n",
    "\n",
    "print(f\"No LM Transcript: {ctc_nolm_transcript}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the language model\n",
    "\n",
    "The language model is also expected to have a strong impact onto performance, since it guides the decoder towards more likely word sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 1**</span>. Compare the CTC decoder using `librispeech-4-gram` files (lexicon, token, and language model downloaded above in this script) and the greedy decoder. To that end, for each decoder, perform ASR on the whole dataset (feel free to reuse/adapt code from the previous lab) and compute the mean WER. Can you interprete the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(directory, pattern='*.wav'):\n",
    "    \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(filename)\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "\n",
    "def process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=True, max_files=None):\n",
    "\n",
    "    # Get the list of files in the dataset folder\n",
    "    audio_files = find_files(data_speech_dir)\n",
    "\n",
    "    # Take a subset of files\n",
    "    nfiles = len(audio_files)\n",
    "    if max_files:\n",
    "        nfiles = min(nfiles, max_files)\n",
    "    audio_files = audio_files[:nfiles]\n",
    "\n",
    "    # Initialize lists containing true and estimated transcripts, as well as WER\n",
    "    true_transcript_all = []\n",
    "    est_transcript_all = []\n",
    "    wer_all = []\n",
    "\n",
    "    for iaf, audio_file in enumerate(audio_files):\n",
    "        \n",
    "        # Get files path\n",
    "        audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "        transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "\n",
    "        # Load an audio signal\n",
    "        waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "\n",
    "        # Apply acoustic model and decoder\n",
    "        with torch.inference_mode():\n",
    "            emission, _ = acoustic_model(waveform)\n",
    "            ctc_decoder_results = decoder(emission)[0][0]\n",
    "\n",
    "        ctc_tokens = decoder.idxs_to_tokens(ctc_decoder_results.tokens)\n",
    "        est_transcript = \"\".join(ctc_tokens).replace(\"|\", \" \").strip().split()\n",
    "        est_transcript = [w.lower() for w in est_transcript]\n",
    "\n",
    "        # Load the true transcription\n",
    "        true_transcript = get_true_transcript(transc_file_path)\n",
    "        \n",
    "        # Compute WER\n",
    "        wer = get_wer(true_transcript, est_transcript)\n",
    "        wer_all.append(wer)\n",
    "\n",
    "        est_transcript_all.append(est_transcript)\n",
    "        true_transcript_all.append(true_transcript)\n",
    "        \n",
    "        # Display results\n",
    "        if verbose:\n",
    "            print(f\"File {iaf+1} / {nfiles}\")\n",
    "            print('Estimated transcript: ', est_transcript)\n",
    "            print('True transcript: ', true_transcript)\n",
    "            print(f\"WER: {wer*100:.1f} %\")\n",
    "\n",
    "    wer_mean = torch.FloatTensor(wer_all).mean()\n",
    "\n",
    "    return wer_mean, est_transcript_all, true_transcript_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC decoder with the 4-gram language model\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=path_lm_lexicon,\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"LM: --- WER: {wer_mean*100:.1f} %\")\n",
    "\n",
    "# Greedy decoder\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=None,\n",
    "    tokens=labels,\n",
    "    lm=None,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"Greedy Decoder --- WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the lexicon\n",
    "\n",
    "The lexicon is expected to have a strong influence on ASR performance, since it constrains the decoder to produce only words that belong to this lexicon, therefore avoiding to procude words that potentially do not exist in a language or given corpus. Here, we propose to use a custom lexicon that only contains words that are in the dataset.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 2**</span>. Perform ASR using such a custom lexicon. To that end:\n",
    "- Build a vocabulary (list of words) from the transcript files in the dataset (load the transcripts and remove duplicates).\n",
    "- Filter the downloaded lexicon by only keeping words from the vocabulary. Write this custom lexicon as a `.txt` file.\n",
    "- Create a dedocer that uses the 4-gram language model and this custom lexicon. Perform ASR on the dataset / compute the WER. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction to get a flat list from a list of list\n",
    "def flatten_list(list_of_lists):\n",
    "    return [x for L in list_of_lists for x in L]\n",
    "\n",
    "# Read all the transcripts in the dataset\n",
    "transcr_files = find_files(data_transc_dir, pattern='*.txt')\n",
    "true_transcript = []\n",
    "for f in transcr_files:\n",
    "    transc_file_path = os.path.join(data_transc_dir, f)\n",
    "    true_transcript.append(get_true_transcript(transc_file_path))\n",
    "\n",
    "# Flatten the list and remove duplicates\n",
    "vocab_dataset = list(set(flatten_list(true_transcript)))\n",
    "print('Words in the dataset:', len(vocab_dataset))\n",
    "\n",
    "# Filter the provided lexicon by keeping only words in our dataset\n",
    "custom_lexicon = []\n",
    "for w in vocab_dataset:\n",
    "    wl = [line for line in open(files.lexicon) if line.startswith(w + '\\t')]\n",
    "    custom_lexicon.append(wl)\n",
    "\n",
    "# again, flatten it\n",
    "custom_lexicon = flatten_list(custom_lexicon)\n",
    "\n",
    "# There are less entries in the lexicon than in our list of words from the dataset: several words are indeed not registered in the lexicon (we could add them manually)\n",
    "print('Entries in the custom lexicon: ', len(custom_lexicon))\n",
    "\n",
    "# Record this lexicon\n",
    "with open(\"mylexicon.txt\", \"w\") as f:\n",
    "    f.writelines(custom_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC decoder with the language model + a custom lexicon\n",
    "decoder = ctc_decoder(\n",
    "    lexicon=\"mylexicon.txt\",\n",
    "    tokens=path_lm_tokens,\n",
    "    lm=path_lm,\n",
    ")\n",
    "wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "print(f\"Custom lexicon --- WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: This custom lexicon produces a better transcript (lower WER) than the more general lexicon. This makes sense since our custom lexicon is specifically tailored for this dataset, and then words outside the dataset cannot be predicted. Nonetheless there are still some errors: besides the acoustic model, these can be due to several words in the dataset not being actually in the original downloaded lexicon, therefore the model cannot predict them using solely the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search parameters\n",
    "\n",
    "The beam search algorithm used in the CTC decoder depends on other parameters, such as `nbest` which determines the number of hypotheses to return, or `lm_weight` which adjust the relative importance of the language model vs. the acoustic model predictions. Here we only focus on `beam_size`, which determines the maximum number of best hypotheses to hold after each decoding step. Using larger beam sizes allows for exploring a larger range of possible hypotheses which can produce hypotheses with higher scores, which really is [the core](https://distill.pub/2017/ctc/) of the beam search algorithm.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 3**</span>. Perform ASR on the whole dataset folder for several values of the beam search size parameter: `beam_size` $\\in [1, 10, 100]$ (use the original downloaded lexicon, not the custom one). Compute the WER and the computation time (e.g., via the [time](https://docs.python.org/3/library/time.html#time.time) package). What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "beam_sizes = [1, 10, 100]\n",
    "\n",
    "for beam_size in beam_sizes:\n",
    "    decoder = ctc_decoder(\n",
    "        lexicon=path_lm_lexicon,\n",
    "        tokens=path_lm_tokens,\n",
    "        lm=path_lm,\n",
    "        beam_size=beam_size,\n",
    "    )\n",
    "\n",
    "    time_start = time.time()\n",
    "    wer_mean = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "    time_ellapsed = time.time() - time_start\n",
    "\n",
    "    print(f\"Beam search size: {beam_size} --- WER: {wer_mean*100:.1f} % --- Time: {time_ellapsed:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Using a beam search size that is too large (i.e., building too many hypothesis when decoding) does not improve performance compared to a lower value, but increases the computational cost. Thus, it is recommended to adjust this value in order to find a tradeoff between accuracy and speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation / alignment\n",
    "\n",
    "**Unfinished / non working : can be changed to add an extra exercice if needed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctc_decoder_result.timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = {}\n",
    "word['start'] = 0\n",
    "word['end'] = 10\n",
    "word['label'] = 'l'\n",
    "word_segments = []\n",
    "word_segments.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: faire coder le time end (en utilisant '|' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignments_wav(waveform, emission, tokens, timesteps, sample_rate):\n",
    "    waveform /= waveform.max()\n",
    "    t = torch.arange(waveform.size(0)) / sample_rate\n",
    "    ratio = waveform.size(0) / emission.size(1) / sample_rate\n",
    "\n",
    "    chars = []\n",
    "    words = []\n",
    "    word_start = None\n",
    "    for token, timestep in zip(tokens, timesteps * ratio):\n",
    "        if token == \"|\":\n",
    "            if word_start is not None:\n",
    "                words.append((word_start, timestep))\n",
    "            word_start = None\n",
    "        else:\n",
    "            chars.append((token, timestep))\n",
    "            if word_start is None:\n",
    "                word_start = timestep\n",
    "\n",
    "    fig = plt.figure(figsize=(12,4))\n",
    "    plt.plot(t, waveform)\n",
    "    for token, timestep in chars:\n",
    "        plt.annotate(token, (timestep, 1))\n",
    "    for word_start, word_end in words:\n",
    "        plt.axvspan(word_start, word_end, alpha=0.1, color=\"red\")\n",
    "    plt.yticks([])\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ctc_nolm_tokens = decoder.idxs_to_tokens(ctc_nolm_result.tokens)\n",
    "plot_alignments_wav(waveform[0], emission, ctc_nolm_tokens, ctc_decoder_result.timesteps, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_alignments(segments, word_segments, waveform, sample_rate=bundle.sample_rate):\n",
    "\n",
    "    # The original waveform\n",
    "    ratio = waveform.size(0) / sample_rate / trellis.size(0)\n",
    "    ax2.specgram(waveform, Fs=sample_rate)\n",
    "    for word in word_segments:\n",
    "        x0 = ratio * word.start\n",
    "        x1 = ratio * word.end\n",
    "        ax2.axvspan(x0, x1, facecolor=\"none\", edgecolor=\"white\", hatch=\"/\")\n",
    "        ax2.annotate(f\"{word.score:.2f}\", (x0, sample_rate * 0.51), annotation_clip=False)\n",
    "\n",
    "    for seg in segments:\n",
    "        if seg.label != \"|\":\n",
    "            ax2.annotate(seg.label, (seg.start * ratio, sample_rate * 0.55), annotation_clip=False)\n",
    "    ax2.set_xlabel(\"time [second]\")\n",
    "    ax2.set_yticks([])\n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "plot_alignments(\n",
    "    segments,\n",
    "    word_segments,\n",
    "    waveform[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_segment(waveform, word, num_time_steps, sr=16000):\n",
    "    ratio = waveform.shape[-1] / num_time_steps\n",
    "    x0 = int(ratio * word['start'])\n",
    "    x1 = int(ratio * word['end'])\n",
    "    print(f\"Character: {word['label']}\")\n",
    "    segment = waveform[:, x0:x1]\n",
    "    return IPython.display.Audio(segment, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = word_segments[0]\n",
    "display_segment(waveform, word, num_time_steps, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
