{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Automatic Speech Recognition\n",
    "\n",
    "**Name**: \n",
    "\n",
    "This lab is an introduction to automatic speech recognition (ASR). In a nutshell, the ASR process consists of the following blocks:\n",
    "\n",
    "1. **Feature extraction**: The audio waveform is preprocessed into normalized features such as spectrogram or MFCCs.\n",
    "2. **Acoustic model**: The features are transformed into high-dimensional embeddings via a deep neural network (here: [wav2vec2](https://arxiv.org/pdf/2006.11477)), and then a classifier is applied to predict the class of each token (=character) at each time step.\n",
    "3. **Decoding**: A transcript is generated from the sequence of class probabilities, either using a language model or not.\n",
    "\n",
    "<center><a href=\"https://developer.nvidia.com/blog/how-to-build-domain-specific-automatic-speech-recognition-models-on-gpus/\">\n",
    "    <img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2019/12/ASR-Pipeline-1.png\" width=\"600\"></a></center>\n",
    "    \n",
    "In this lab, we experiment with ASR using pretrained acoustic models (which include feature extraction for simplicity). We provide a dataset of real-life speech audio (with trancription) in order to study the influence of several factors onto the performance of a modern ASR system. We focus on the usage of the acoustic model, thus we use a simple decoder with no language model (this will be the topic of the next lab).\n",
    "\n",
    "This series of lab relies on [torchaudio](https://pytorch.org/audio/stable/index.html), which is a pytorch-based librairy for audio/signal processing. More specifically, this lab is based on this [tutorial](https://pytorch.org/audio/stable/tutorials/speech_recognition_pipeline_tutorial.html#sphx-glr-tutorials-speech-recognition-pipeline-tutorial-py), which you are encouraged to check.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import IPython\n",
    "import os\n",
    "import fnmatch\n",
    "import matplotlib.pyplot as plt\n",
    "torch.random.manual_seed(0);\n",
    "\n",
    "MAX_FILES = 100 # lower this number for processing a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dataset path - If needed, you can change it HERE but NOWHERE ELSE in the notebook!\n",
    "data_dir = \"../dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech and transcripts sub-directories paths\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and display an audio example\n",
    "\n",
    "Let us first load an audio file, listen to it using an audio reader, and plot the waveform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example file\n",
    "audio_file = '61-70968-0001.wav'\n",
    "audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "print(f\"Audio file path: {audio_file_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the speech signal waveform\n",
    "plt.figure()\n",
    "xt = torch.arange(waveform.size(1)) / sr\n",
    "plt.plot(xt, waveform.T)\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acoustic model\n",
    "\n",
    "Let us create an acoustic model that performs feature extraction and classification. In this lab, we use [wav2vec2](https://arxiv.org/pdf/2006.11477), a self-supervised audio model that produces state-of-the-art results for many audio tasks (including ASR).\n",
    "\n",
    "For convenience, both the model definition and pretrained wav2vec2 weights can be directly loaded using torchaudio, which includes two types of weights:\n",
    "- The pretrained weights [without fine-tuning](https://pytorch.org/audio/stable/pipelines.html#id3), that can be fine-tuned for any other downstream tasks (e.g., emotion recognition or language detection).\n",
    "- The pretrained weights that are [fine-tuned for the ASR task](https://pytorch.org/audio/stable/pipelines.html#id36), using an extra classification layer after feature extraction with wav2vec2.\n",
    "\n",
    "In this series of lab, we use the latter. Below we provide the code for constructing a model and fetching the pretrained weights along with the classification labels. Here we use the `WAV2VEC2_ASR_BASE_100H` model. As the name indicates, it is a basic wav2vec2 architecture that is fine-tuned for ASR using 100 hours of the Librispeech dataset (containing both speech and transcriptions). Other models can be fetched using torchaudio (see the list [here](https://pytorch.org/audio/stable/pipelines.html#id36))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the acoustic model\n",
    "model_name = 'WAV2VEC2_ASR_BASE_100H'\n",
    "bundle = getattr(torchaudio.pipelines, model_name)\n",
    "acoustic_model = bundle.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the details of the model's architecture (more info on this in the Neural Network labs!)\n",
    "print(acoustic_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and display the classification labels\n",
    "labels = bundle.get_labels()\n",
    "print('Labels:', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are the characters that can be predicted by the acoustic model. Note that there are two special labels:\n",
    "- `-` is the blank label, which corresponds to no character.\n",
    "- `|` is the end-of-word label, which is useful to further segment a list of characters into a sequence of words.\n",
    "\n",
    "We can now apply the acoustic model to the example audio signal wa have loaded above. We can either compute the acoustic features or predicting the labels after the classification layer (also called the *emission* matrix/tensor):\n",
    "- `features` is a list of tensors, where each tensor is the output of a transformer layer from the wav2vec2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    features, _ = acoustic_model.extract_features(waveform)\n",
    "\n",
    "print(f\"Number of transformer layers: {len(features)}\")\n",
    "print(features[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `emission` is a tensor of size `[1, time steps, number of classes]` corresponding to the output of the classification layer (`1` corresponds to the batch size). Note that it is in the form of logits, not probabilities (thus it is not normalized in $[0, 1]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    emission, _ = acoustic_model(waveform)\n",
    "\n",
    "print(emission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Vizualize the result of the classification\n",
    "plt.figure()\n",
    "plt.imshow(emission[0].cpu().T)\n",
    "plt.title(\"Classification result\")\n",
    "plt.xlabel(\"Frame (time-axis)\")\n",
    "plt.ylabel(\"Class\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the most likely label at each time frame to get a first rough transcript estimate, as done below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.argmax(emission[0], dim=-1)  # take the most likely index at each time step\n",
    "pred_labels = [labels[i] for i in indices] # transform each index into the corresponding label\n",
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are many blank labels `'-'`, as well as duplicates. This is why some further postprocessing / decoding is needed.\n",
    "\n",
    "## Decoding\n",
    "\n",
    "From the emission tensor, which is the sequence of probabilities (or logits) for each class / token, we now want to generate a transcript (or *hypothesis*). This process is called *decoding*. Decoding can be complex because it implies accounting for the context in order to avoid duplicates, assembling characters into words, identifying precise word start / end, and building sequences of words that make sense given some underlying language model.\n",
    "\n",
    "Such refined decoding techniques involve using external ressources such as lexicon and language models. In this lab however, we rely on a naive approach called a *greedy* decoder, which does not depend on such components. The context information is not used (= we pick the best hypothesis at each time step), and only one transcript is generated by applying some basic post-processing (removing blank tokens and duplicates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class GreedyDecoder(torch.nn.Module):\n",
    "    def __init__(self, labels, blank_token_indx=0):\n",
    "        super().__init__()\n",
    "        self.labels = labels\n",
    "        self.blank_token_indx = blank_token_indx\n",
    "\n",
    "    def forward(self, emission):\n",
    "        \"\"\"Given a sequence emission over labels, decode the transcript\n",
    "        Args:\n",
    "          emission (Tensor): Logit tensors. Shape `[1, num_seq, num_label]`.\n",
    "        Returns:\n",
    "          transcript (List of strings): The resulting transcript\n",
    "        \"\"\"\n",
    "        emission = emission[0] # take the first element in the batch (only one)\n",
    "        indices = torch.argmax(emission, dim=-1)  # take the most likely index at each time step\n",
    "        indices = torch.unique_consecutive(indices, dim=-1) # remove duplicates\n",
    "        indices = [i for i in indices if i != self.blank_token_indx] # remove blank token\n",
    "        transcript = \"\".join([self.labels[i] for i in indices]) # convert indices into tokens\n",
    "        transcript = transcript.replace(\"|\", \" \").lower().split() # a bit of post-processing\n",
    "        return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Instanciate the decoder, apply it, and display the result\n",
    "decoder = GreedyDecoder(labels=labels)\n",
    "est_transcript = decoder(emission)\n",
    "print(est_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "To evaluate the quality of the transcript above, we need to compare it to some reference. More precisely, we can compute the [word error rate](https://en.wikipedia.org/wiki/Word_error_rate) (WER) between the reference and estimated transcripts. The WER is a measure of speech recognition performance expressed in % (lower is better) based on the Levenshtein distance, which allows to account for the fact that the true and estimated transcripts might have different lengths; more info in the course.\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 1**</span>. Define the path to the true transcript file - the transcript file name is the same as the audio, except for the extension (`.txt` instead of `.wav`). Load the content of this file to get the true transcript, and preprocess it to the same format as the estimated transcript above (i.e., use the `.lower()` and `.split()` methods). Display the true transcript. Then, using the provided `get_wer` function, compute and display the WER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wer(true_transcript, est_transcript):\n",
    "    wer = torchaudio.functional.edit_distance(true_transcript, est_transcript) / len(true_transcript)\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_transcript(transc_file_path):\n",
    "    with open(transc_file_path, \"r\") as f:\n",
    "        true_transcript = f.read()\n",
    "    true_transcript = true_transcript.lower().split()\n",
    "    return true_transcript\n",
    "    \n",
    "# Load and display the true transcription\n",
    "transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "true_transcript = get_true_transcript(transc_file_path)\n",
    "print(true_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and display the WER score\n",
    "wer = get_wer(true_transcript, est_transcript)\n",
    "print(f\"WER: {wer*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process a folder\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 2**</span>. Process the entire dataset folder (=the 100 files) using the wav2vec2 acoustic model and the greedy decoder above, and compute and display the mean WER over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide a function below that returns the list of all files in a directory\n",
    "def find_files(directory, pattern='*.wav'):\n",
    "    \"\"\"Recursively finds all files matching the pattern.\"\"\"\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(directory):\n",
    "        for filename in fnmatch.filter(filenames, pattern):\n",
    "            files.append(filename)\n",
    "    files = sorted(files)\n",
    "    return files\n",
    "\n",
    "# Get the list of audio files in the asr dataset\n",
    "audio_files = find_files(data_speech_dir)\n",
    "print(audio_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a folder\n",
    "def process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=True, max_files=None):\n",
    "\n",
    "    # Get the list of files in the dataset folder\n",
    "    audio_files = find_files(data_speech_dir)\n",
    "\n",
    "    # Take a subset of files\n",
    "    nfiles = len(audio_files)\n",
    "    if max_files:\n",
    "        nfiles = min(nfiles, max_files)\n",
    "    audio_files = audio_files[:nfiles]\n",
    "\n",
    "    # Initialize lists containing true and estimated transcripts, as well as WER\n",
    "    true_transcript_all = []\n",
    "    est_transcript_all = []\n",
    "    wer_all = []\n",
    "\n",
    "    for iaf, audio_file in enumerate(audio_files):\n",
    "        \n",
    "        # Get files path (speech and transcript)\n",
    "        audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "        transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "\n",
    "        # Load an audio signal\n",
    "        waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "\n",
    "        # Apply acoustic model and decoder\n",
    "        with torch.inference_mode():\n",
    "            emission, _ = acoustic_model(waveform)\n",
    "            est_transcript = decoder(emission)\n",
    "            \n",
    "        # Load the true transcription\n",
    "        true_transcript = get_true_transcript(transc_file_path)\n",
    "        \n",
    "        # Compute WER\n",
    "        wer = get_wer(true_transcript, est_transcript)\n",
    "        wer_all.append(wer)\n",
    "\n",
    "        est_transcript_all.append(est_transcript)\n",
    "        true_transcript_all.append(true_transcript)\n",
    "        \n",
    "        # Display results\n",
    "        if verbose:\n",
    "            print(f\"File {iaf+1} / {nfiles}\")\n",
    "            print('Estimated transcript: ', est_transcript)\n",
    "            print('True transcript: ', true_transcript)\n",
    "            print(f\"WER: {wer*100} %\")\n",
    "\n",
    "    wer_mean = torch.FloatTensor(wer_all).mean()\n",
    "\n",
    "    return wer_mean, est_transcript_all, true_transcript_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the dataset and display the mean WER\n",
    "wer_mean, _, _ = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)\n",
    "print(f\"Mean WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the noise\n",
    "\n",
    "ASR systems are expected to be sensitive to the noise: while a model might work well on a clean recording, its performance will drop in real-life conditions where speech is contaminated with noise. We can simulate such a scenario by adding white noise to our speech waveforms.\n",
    "\n",
    "The amount of noise is controlled by the *signal-to-noise ratio* (SNR), expressed in dB. The lower the SNR, the noisier the mixture: a clean speech signal corresponds to SNR=+$\\infty$, while a pure noise signal corresponds to SNR=-$\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions below allow to generate white noise, and add it to the clean speech at a certain SNR level\n",
    "\n",
    "def adjust_noise_snr(\n",
    "    speech,\n",
    "    noise,\n",
    "    snr_dB,\n",
    "    silence_threshold=0.01,\n",
    "):\n",
    "\n",
    "    # Filter out silence from speech signal\n",
    "    speech_wo_silence = speech[torch.abs(speech) > silence_threshold]\n",
    "\n",
    "    # Calculate the total energy of the speech and noise signals\n",
    "    speech_power = torch.sum(torch.square(speech_wo_silence))\n",
    "    noise_power = torch.sum(torch.square(noise))\n",
    "\n",
    "    # Compute gain factor to achieve desired SNR\n",
    "    gain_factor = torch.sqrt((speech_power / noise_power) * 10 ** (-snr_dB / 10))\n",
    "\n",
    "    # Scale the noise accordingly\n",
    "    scaled_noise = gain_factor * noise\n",
    "\n",
    "    return scaled_noise\n",
    "\n",
    "\n",
    "def add_noise(speech, snr_dB=0):\n",
    "    # Generate random white noise\n",
    "    noise = torch.randn_like(speech)\n",
    "    # Adjust noise level\n",
    "    noise = adjust_noise_snr(speech, noise, snr_dB)\n",
    "    # Add the noise to the clean speech\n",
    "    speech_noisy = speech + noise\n",
    "    return speech_noisy, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the original audio signal at SNR = 0 dB\n",
    "speech_noisy_0dB, noise_OdB = add_noise(waveform, snr_dB=0)\n",
    "IPython.display.Audio(data=speech_noisy_0dB, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise to the original audio signal at SNR = 5 dB\n",
    "speech_noisy_5dB, noise_5dB = add_noise(waveform, snr_dB=5)\n",
    "IPython.display.Audio(data=speech_noisy_5dB, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the noisy signals (vs. the clean one)\n",
    "plt.figure()\n",
    "xt = torch.arange(waveform.size(1)) / sr\n",
    "plt.plot(xt, speech_noisy_0dB.T, label='noisy, SNR=0 dB')\n",
    "plt.plot(xt, speech_noisy_5dB.T, label='noisy, SNR=5 dB')\n",
    "plt.plot(xt, waveform.T, label='clean')\n",
    "plt.yticks([])\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 3**</span>. For each SNR $\\in [-5, 0, 5, 10]$ dB, add noise to the files in the dataset, perform ASR and compute/display the mean WER. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a folder\n",
    "def process_folder_noisy(data_speech_dir, data_transc_dir, acoustic_model, decoder, snr_dB=0, verbose=True, max_files=None):\n",
    "\n",
    "    # Get the list of files in the dataset folder\n",
    "    audio_files = find_files(data_speech_dir)\n",
    "\n",
    "    # Take a subset of files\n",
    "    nfiles = len(audio_files)\n",
    "    if max_files:\n",
    "        nfiles = min(nfiles, max_files)\n",
    "    audio_files = audio_files[:nfiles]\n",
    "\n",
    "    # Initialize lists containing true and estimated transcripts, as well as WER\n",
    "    true_transcript_all = []\n",
    "    est_transcript_all = []\n",
    "    wer_all = []\n",
    "\n",
    "    for iaf, audio_file in enumerate(audio_files):\n",
    "        \n",
    "        # Get files path\n",
    "        audio_file_path = os.path.join(data_speech_dir, audio_file)\n",
    "        transc_file_path = os.path.join(data_transc_dir, audio_file.replace('wav', 'txt'))\n",
    "\n",
    "        # Load an audio signal\n",
    "        waveform, sr = torchaudio.load(audio_file_path, channels_first=True)\n",
    "\n",
    "        # Add noise\n",
    "        waveform, _ = add_noise(waveform, snr_dB)\n",
    "        \n",
    "        # Apply acoustic model and decoder\n",
    "        with torch.inference_mode():\n",
    "            emission, _ = acoustic_model(waveform)\n",
    "            est_transcript = decoder(emission)\n",
    "            \n",
    "        # Load the true transcription\n",
    "        true_transcript = get_true_transcript(transc_file_path)\n",
    "        \n",
    "        # Compute WER\n",
    "        wer = get_wer(true_transcript, est_transcript)\n",
    "        wer_all.append(wer)\n",
    "\n",
    "        est_transcript_all.append(est_transcript)\n",
    "        true_transcript_all.append(true_transcript)\n",
    "        \n",
    "        # Display results\n",
    "        if verbose:\n",
    "            print(f\"File {iaf+1} / {nfiles}\")\n",
    "            print('Estimated transcript: ', est_transcript)\n",
    "            print('True transcript: ', true_transcript)\n",
    "            print(f\"WER: {wer*100} %\")\n",
    "\n",
    "    wer_mean = torch.FloatTensor(wer_all).mean()\n",
    "\n",
    "    return wer_mean, est_transcript_all, true_transcript_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform ASR at various SNR levels\n",
    "snr_list = [-5, 0, 5, 10]\n",
    "\n",
    "for snr_dB in snr_list:\n",
    "    wer_mean, _, _ = process_folder_noisy(data_speech_dir, data_transc_dir, acoustic_model, decoder, snr_dB=snr_dB, verbose=False, max_files=MAX_FILES)\n",
    "    print(f\"SNR level: {snr_dB} dB ---- Mean WER: {wer_mean*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: We observe that when the level of noise increases (=lower SNR), the ASR performance decreases (=higher WER). This is in line with our expectation, since when speech is contaminated with noise, it becomes harder to understand. This further motivates to build ASR systems that are robust to noise, e.g., via applying a prior *speech enhancement* algorithm to clean up the signal / remove the noise, or training the ASR system on noisy signals directly, so it learn to process the noise in an end-to-end fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of the amount of training data\n",
    "\n",
    "So far, we have used a model that was fine-tuned for ASR on 100 hours of speech, but alternative models exist in torchaudio (recall the list of pipelines is [here](https://pytorch.org/audio/stable/pipelines.html#id36)).\n",
    "\n",
    "<span style=\"color:red\"> **Exercise 4**</span>. Evaluate the impact of the amount of data for fine-tuning. To that end, load models that use the same architecture (= basic wav2vec), but fine-tuned with more or loss speech data, perform ASR on the dataset, and compute the WER. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['WAV2VEC2_ASR_BASE_10M',\n",
    "              'WAV2VEC2_ASR_BASE_100H',\n",
    "              'WAV2VEC2_ASR_BASE_960H']\n",
    "\n",
    "for model_name in model_list:\n",
    "    bundle = getattr(torchaudio.pipelines, model_name)\n",
    "    acoustic_model = bundle.get_model()\n",
    "    decoder = GreedyDecoder(labels=bundle.get_labels())\n",
    "    wer = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=MAX_FILES)[0]\n",
    "    print(f\"Model: {model_name} --- WER: {wer*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment**: Quite intuitively, the more data for fine-tuning, the better the performance. This highlights that while using a powerful word embedding system (wav2vec2) is important, performing some fine-tuning for a specific task is also crucial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that many other pretrained models are readily available in torchaudio, such as a:\n",
    "- larger models (e.g., `'WAV2VEC2_ASR_LARGE_10M'`)\n",
    "- model trained using another dataset (e.g., `'WAV2VEC2_ASR_LARGE_LV60K_10M'`)\n",
    "- alternative architectures (e.g., `'HUBERT_ASR_LARGE'`).\n",
    "\n",
    "Feel free to experiment with these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different training dataset / model architecture\n",
    "#model_list = ['WAV2VEC2_ASR_LARGE_LV60K_10M', # dataset (LV60k)\n",
    "#              'WAV2VEC2_ASR_LARGE_LV60K_100H',\n",
    "#              'WAV2VEC2_ASR_LARGE_LV60K_960H',\n",
    "#              'WAV2VEC2_ASR_LARGE_10M', # large wav2vec2\n",
    "#              'WAV2VEC2_ASR_LARGE_100H',\n",
    "#              'WAV2VEC2_ASR_LARGE_960H',\n",
    "#              'HUBERT_ASR_LARGE', # alt architecture to be compared in terms of dataset with WAV2VEC2_ASR_LARGE_LV60K_960H\n",
    "#             ]\n",
    "\n",
    "#for model_name in model_list:\n",
    "#    bundle = getattr(torchaudio.pipelines, model_name)\n",
    "#    acoustic_model = bundle.get_model()\n",
    "#    decoder = GreedyDecoder(labels=bundle.get_labels())\n",
    "#    wer = process_folder(data_speech_dir, data_transc_dir, acoustic_model, decoder, verbose=False, max_files=100)[0]\n",
    "#    print(f\"Model: {model_name} --- WER: {wer*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To remove; it's just a backup using 'speechbrain'\n",
    "\n",
    "#from speechbrain.inference.ASR import EncoderDecoderASR\n",
    "#model_name = \"asr-crdnn-rnnlm-librispeech\"\n",
    "#asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/\" + model_name,\n",
    "#                                           savedir=model_dir + model_name)\n",
    "#def compute_transcription(asr_model, signal, sr):\n",
    "#    # Normalize it as for inputing to the ASR model\n",
    "#    signal_norm = asr_model.audio_normalizer(signal, sr)\n",
    "#    # Fake a batch\n",
    "#    batch = signal_norm.unsqueeze(0)\n",
    "#    # Compute the transcription with the model\n",
    "#    predicted_words, _ = asr_model.transcribe_batch(\n",
    "#        batch, torch.tensor([1.0])\n",
    "#    )\n",
    "#    return predicted_words[0]\n",
    "#    \n",
    "#transcribed_speech = compute_transcription(asr_model, speech, sr)\n",
    "#print(transcribed_speech)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
