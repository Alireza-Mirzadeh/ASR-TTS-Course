{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Text-to-Speech (TTS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:darkblue\"> Introduction </span>\n",
    "\n",
    "\n",
    "In this lab, first, we will study how to build and evaluate a neural-based TTS system using a pretrained Tacotron 2 in torchaudio and different vocoders. Then, we will use a multi-speaker TTS model for experiments with different voices using SpeechBrain pretrained models.\n",
    "\n",
    "\n",
    "The TTS pipeline comprises 3 steps:\n",
    "\n",
    "<img src=\"tts.png\">\n",
    "\n",
    "\n",
    "#### <span style=\"color:green\"> Step 1. Text processing </span>\n",
    "\n",
    "The input text is encoded into a list of symbols. We will use English characters and phonemes as the symbols. A phonemizer transforms text into phoneme sequences. Phonemes are textual representations of the pronunciation of words. \n",
    "\n",
    "#### <span style=\"color:green\"> Step 2. Spectrogram generation </span>\n",
    "\n",
    "From the encoded text, a spectrogram is generated. We use the ``Tacotron 2``\n",
    "model for this. \n",
    "\n",
    "#### <span style=\"color:green\"> Step 3. Conversion of the spectrogram into a waveform (speech generation) </span>\n",
    "\n",
    "The spectrogram is converted into a speech waveform using a Vocoder.\n",
    "In this lab, three different vocoders are used,\n",
    "   :py:class:`~torchaudio.models.WaveRNN`,\n",
    "   :py:class:`~torchaudio.transforms.GriffinLim`, and\n",
    "   [Nvidia's WaveGlow](https://pytorch.org/hub/nvidia_deeplearningexamples_tacotron2/).\n",
    "\n",
    "\n",
    "The related components are bundled in :py:class:`torchaudio.pipelines.Tacotron2TTSBundle`.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block diagram of the Tacotron 2 system architecture\n",
    "\n",
    "<img src=\"https://pytorch.org/assets/images/tacotron2_diagram.png\" width=\"500\">\n",
    "\n",
    "The Tacotron 2 and WaveGlow model form a TTS system that enables user to synthesise a natural sounding speech from raw transcripts without any additional prosody information. \n",
    "The Tacotron 2 model produces mel spectrograms from input text using encoder-decoder architecture. \n",
    "WaveGlow is a flow-based model that consumes the mel spectrograms to generate speech.\n",
    "\n",
    "*Note: \n",
    "    This lab is partly based on the [torchaudio tutorial](https://pytorch.org/audio/stable/tutorials/tacotron2_pipeline_tutorial.html) and [SpeechBrain](https://github.com/speechbrain/speechbrain) examples.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.audio import NonIntrusiveSpeechQualityAssessment\n",
    "from speechbrain.inference.TTS import MSTacotron2\n",
    "from speechbrain.inference.vocoders import HIFIGAN\n",
    "\n",
    "torch.random.manual_seed(0)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech and transcripts sub-directories paths\n",
    "data_dir = \"../dataset\"\n",
    "data_speech_dir = os.path.join(data_dir, 'speech')\n",
    "data_transc_dir = os.path.join(data_dir, 'transcription')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Step 1. Text processing </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character-based encoding\n",
    "\n",
    "In this section, we will learn how character-based encoding works.\n",
    "\n",
    "Since the pre-trained Tacotron 2 model expects specific set of symbol\n",
    "tables, the same functionalities is available in ``torchaudio``. However,\n",
    "we will first manually implement the encoding for better understanding.\n",
    "\n",
    "First, we define a set of symbols\n",
    "``'_-!\\'(),.:;? abcdefghijklmnopqrstuvwxyz'``. Then, we map  each character of the input text into the index of the corresponding symbol in the table. Symbols that are not in the table are ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "symbols = \"_-!'(),.:;? abcdefghijklmnopqrstuvwxyz\"\n",
    "look_up = {s: i for i, s in enumerate(symbols)}\n",
    "symbols = set(symbols)\n",
    "\n",
    "def text_to_sequence(text):\n",
    "    text = text.lower()\n",
    "    return [look_up[s] for s in text if s in symbols]\n",
    "\n",
    "text = \"We are studing text-to-speech models with different vocoders. What are the differences between these models?\"\n",
    "print(text_to_sequence(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the symbol table and indices must match\n",
    "what the pretrained Tacotron 2 model expects. ``torchaudio`` provides the same\n",
    "transform along with the pretrained model. You can\n",
    "instantiate and use such transform as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "processor = torchaudio.pipelines.TACOTRON2_WAVERNN_CHAR_LJSPEECH.get_text_processor()\n",
    "\n",
    "text = \"We are studing text-to-speech models with different vocoders. What are the differences between these models?\"\n",
    "processed, lengths = processor(text)\n",
    "\n",
    "print(processed)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The output of our manual encoding and the ``torchaudio`` ``text_processor`` output matches (meaning we correctly re-implemented what the library does internally). It takes either a text or list of texts as inputs.\n",
    "When a list of texts are provided, the returned ``lengths`` variable\n",
    "represents the valid length of each processed tokens in the output\n",
    "batch.\n",
    "\n",
    "The intermediate representation can be retrieved as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print([processor.tokens[i] for i in processed[0, : lengths[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phoneme-based encoding\n",
    "\n",
    "Phoneme-based encoding is similar to character-based encoding, but it\n",
    "uses a symbol table based on phonemes and a G2P (Grapheme-to-Phoneme)\n",
    "model.\n",
    "\n",
    "The detail of the G2P model is out of the scope of this tutorial, we will\n",
    "just look at what the conversion looks like.\n",
    "\n",
    "Similar to the case of character-based encoding, the encoding process is\n",
    "expected to match what a pretrained Tacotron 2 model is trained on.\n",
    "``torchaudio`` has an interface to create the process.\n",
    "\n",
    "The following code illustrates how to make and use the process. Behind\n",
    "the scene, a G2P model is created using ``DeepPhonemizer`` package, and\n",
    "the pretrained weights published by the author of ``DeepPhonemizer`` is\n",
    "fetched.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "\n",
    "text = \"We are studing text-to-speech models with different vocoders. What are the differences between these models?\"\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "\n",
    "print(processed)\n",
    "print(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the encoded values are different from the example of\n",
    "character-based encoding.\n",
    "\n",
    "The intermediate representation looks as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print([processor.tokens[i] for i in processed[0, : lengths[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Step 2. Spectrogram generation </span>\n",
    "\n",
    "\n",
    "``Tacotron 2`` is the model we use to generate spectrogram from the\n",
    "encoded text. For more details, please refer to [the\n",
    "paper](https://arxiv.org/abs/1712.05884).\n",
    "\n",
    "It is easy to instantiate a Tacotron 2 model with pretrained weights,\n",
    "however, note that the input to Tacotron 2 models need to be processed\n",
    "by the matching text processor.\n",
    "\n",
    ":py:class:`torchaudio.pipelines.Tacotron2TTSBundle` bundles the matching\n",
    "models and processors together so that it is easy to create the pipeline.\n",
    "\n",
    "For the available bundles, and its usage, please refer to\n",
    ":py:class:`~torchaudio.pipelines.Tacotron2TTSBundle`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "\n",
    "text = \"We are studing text-to-speech models with different vocoders. What are the differences between these models?\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, _, _ = tacotron2.infer(processed, lengths)\n",
    "\n",
    "_ = plt.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``Tacotron2.infer`` method perfoms multinomial sampling,\n",
    "therefore, the process of generating the spectrogram incurs randomness.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1)\n",
    "for i in range(3):\n",
    "    with torch.inference_mode():\n",
    "        spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "    print(spec[0].shape)\n",
    "    ax[i].imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Step 3. Waveform generation using different vocoders </span>\n",
    "\n",
    "The obtained spectrogram is used to generate a waveform using a vocoder.\n",
    "\n",
    "``torchaudio`` provides vocoders based on ``GriffinLim`` and ``WaveRNN``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WaveRNN vocoder\n",
    "\n",
    "Continuing from the previous section, we can instantiate the matching\n",
    "WaveRNN model from the same bundle.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_WAVERNN_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "text = \"We are studing text-to-speech models with different vocoders. What are the differences between these models?\"\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "    waveforms, lengths = vocoder(spec, spec_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot(waveforms, spec, sample_rate):\n",
    "    waveforms = waveforms.cpu().detach()\n",
    "\n",
    "    fig, [ax1, ax2] = plt.subplots(2, 1)\n",
    "    ax1.plot(waveforms[0])\n",
    "    ax1.set_xlim(0, waveforms.size(-1))\n",
    "    ax1.grid(True)\n",
    "    ax2.imshow(spec[0].cpu().detach(), origin=\"lower\", aspect=\"auto\")\n",
    "    return IPython.display.Audio(waveforms[0:1], rate=sample_rate)\n",
    "\n",
    "plot(waveforms, spec, vocoder.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Griffin-Lim vocoder\n",
    "\n",
    "The usage of the Griffin-Lim vocoder is similar to WaveRNN. You can instantiate the vocoder object with\n",
    ":py:func:`~torchaudio.pipelines.Tacotron2TTSBundle.get_vocoder`\n",
    "method and pass the spectrogram.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.TACOTRON2_GRIFFINLIM_PHONE_LJSPEECH\n",
    "\n",
    "processor = bundle.get_text_processor()\n",
    "tacotron2 = bundle.get_tacotron2().to(device)\n",
    "vocoder = bundle.get_vocoder().to(device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    processed, lengths = processor(text)\n",
    "    processed = processed.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    spec, spec_lengths, _ = tacotron2.infer(processed, lengths)\n",
    "    waveforms, lengths = vocoder(spec, spec_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plot(waveforms, spec, vocoder.sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waveglow vocoder\n",
    "\n",
    "Waveglow is a vocoder published by Nvidia. The pretrained weights are published on Torch Hub. One can instantiate the model using ``torch.hub`` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Workaround to load model mapped on GPU\n",
    "# https://stackoverflow.com/a/61840832\n",
    "waveglow = torch.hub.load(\n",
    "    \"NVIDIA/DeepLearningExamples:torchhub\",\n",
    "    \"nvidia_waveglow\",\n",
    "    model_math=\"fp32\",\n",
    "    pretrained=False,\n",
    ")\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "    \"https://api.ngc.nvidia.com/v2/models/nvidia/waveglowpyt_fp32/versions/1/files/nvidia_waveglowpyt_fp32_20190306.pth\",  # noqa: E501\n",
    "    progress=False,\n",
    "    map_location=device,\n",
    ")\n",
    "state_dict = {key.replace(\"module.\", \"\"): value for key, value in checkpoint[\"state_dict\"].items()}\n",
    "\n",
    "waveglow.load_state_dict(state_dict)\n",
    "waveglow = waveglow.remove_weightnorm(waveglow)\n",
    "waveglow = waveglow.to(device)\n",
    "waveglow.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    waveforms = waveglow.infer(spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(waveforms, spec, 22050)\n",
    "# Save and display the waverform\n",
    "torchaudio.save(\"synthesized_sample.wav\", waveforms.squeeze(1).cpu(), 22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 1**</span>\n",
    "\n",
    "<span style=\"color:orange\"> **Speech-to-text generation and subjective evaluation**</span>\n",
    "\n",
    "\n",
    "1. Take a fragment of an arbitrary text (a few utterances).\n",
    "2. Synthesize this text using the Tacotron-2 model and 3 different vocoders (Griffin-Lim, WaveRNN, and Waveglow).\n",
    "3. Listen to the obtained synthesised audio samples and evaluate them in terms of speech naturalness and intelligibility using the scale 1..5. See slides #6-8 *Subjective Evaluation* for more details on the criteria of evaluation.\n",
    "4. Based on these scores, perform ranking of the vocoders and describe the problems you observe (mispronunciation, wrong intonation, etc.) for each vocoder regarding speech naturalness and intelligibility and provide audio examples (one for each factor is sufficient).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Objective evaluation </span>\n",
    "\n",
    "### Objective evaluation using ASR\n",
    "\n",
    "Sometimes, instead of human listeners, the ASR model can be used as an analogue for speech intelligibility assessment.\n",
    "\n",
    "The WER metric is computed between the reference text and the output of the ASR system applied to the synthesized speech sample.\n",
    "\n",
    "\n",
    "\n",
    "### Objective evaluation using NISQA (Non Intrusive Speech Quality Assessment)\n",
    "\n",
    "This part is devoted to objective evaluation of speech quality/nauralness using NISQA models.\n",
    "In the current examples, for simplicity, a general NISQA model for speech quality evaluation is used. For evaluaton of TTS results, in practice, a specified NISQA-TTS model should be used.\n",
    "In the output, the first value corresponds to the overal MOS quality estimation that is suggested to use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nisqa = NonIntrusiveSpeechQualityAssessment(22050)\n",
    "\n",
    "waveform, sr = torchaudio.load('synthesized_sample.wav', channels_first=True)\n",
    "nisqa.update(waveform)\n",
    "fig, ax_ = nisqa.plot()\n",
    "fig.savefig(\"nisqa_synthesized_test.png\")\n",
    "\n",
    "# Float tensor with shape (...,5) \n",
    "# corresponding to overall MOS, noisiness, discontinuity, coloration and loudness in that order\n",
    "print(nisqa(waveform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective evaluation using wv-MOS\n",
    "\n",
    "This part is devoted to objective MOS score prediction by the fine-tuned wav2vec2.0 model.\n",
    "\n",
    "**Commented because non-working due to compatibility issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from wvmos import get_wvmos\n",
    "#model = get_wvmos(cuda=False)\n",
    "\n",
    "#mos = model.calculate_one(os.path.join(data_speech_dir, \"237-126133-0009.wav\")) # infer MOS score for one audio\n",
    "#print(mos)\n",
    "#mos = model.calculate_dir(\"path/to/dir/with/wav/files\", mean=True) # infer average MOS score across .wav files in directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 2**</span>\n",
    "\n",
    "<span style=\"color:orange\"> **Objective evaluation** </span>\n",
    "\n",
    "1. Use text transcripts from the text files in */asr-dataset/transcription* (directory from the previous ASR labs). Select a subsample of 6-20 files, preferably including all different speakers of the original dataset (different speakers have different first numbers in the file names, i.e. file 121-121726-0002.txt corresponds to speaker #121, file 61-70968-0018.txt - to speaker #61). Synthesize speech for these files three times using three different vocoders.\n",
    "2. Compute MOS naturalness and MOS quality predictions using neural network wv-MOS and NISQA models and compare them.\n",
    "3. Do you observe similar or different ranking of the TTS systems when using subjective (in Exercise 1) and objective scores?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Multi-speaker TTS </span>\n",
    "\n",
    "This part is devoted to Zero-Shot Multi-Speaker TTS with SpeechBrain toolkit using a variation of Tacotron 2, extended to incorporate speaker identity information when generating speech. It is pretrained on the [LibriTTS](https://openslr.org/60/) corpus (multi-speaker English corpus of approximately 585 hours of read English).\n",
    "\n",
    "*Note*:\n",
    "    The model generates speech at a rate of 22050 Hz, but it's important to note that the input signal, crucial for capturing speaker identities, must be sampled at 16kHz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Intialize TTS (mstacotron2) and Vocoder (HiFIGAN)\n",
    "ms_tacotron2 = MSTacotron2.from_hparams(source=\"speechbrain/tts-mstacotron2-libritts\", savedir=\"pretrained_models/tts-mstacotron2-libritts\")\n",
    "hifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-libritts-22050Hz\", savedir=\"pretrained_models/tts-hifigan-libritts-22050Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of reference files for different speakers\n",
    "# audio_file_reference = '121-127105-0006.wav' # female voice\n",
    "# audio_file_reference = '61-70968-0004.wav' # male voice\n",
    "audio_file_reference = '237-126133-0009.wav' # female voice\n",
    "audio_file_reference_path = os.path.join(data_speech_dir, audio_file_reference)\n",
    "print(f\"Audio file reference path: {audio_file_reference_path}\")\n",
    "\n",
    "waveform, sr = torchaudio.load(audio_file_reference_path, channels_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IPython.display.Audio(data=waveform, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello! This is a multi-speaker text-to-speech model. We generate a test sample.\"\n",
    "\n",
    "# Running the Zero-Shot Multi-Speaker Tacotron2 model to generate mel-spectrogram\n",
    "mel_outputs, mel_lengths, alignments = ms_tacotron2.clone_voice(text, audio_file_reference_path)\n",
    "\n",
    "# Running Vocoder (spectrogram-to-waveform)\n",
    "waveforms = hifi_gan.decode_batch(mel_outputs)\n",
    "\n",
    "# Save and display the waverform\n",
    "torchaudio.save(\"synthesized_sample.wav\", waveforms.squeeze(1).cpu(), 22050)\n",
    "IPython.display.Audio(data=waveforms.squeeze(1), rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> **Exercise 3**</span>\n",
    "\n",
    "<span style=\"color:orange\"> **Multi-speaker TTS** </span>\n",
    "\n",
    "\n",
    "1. Use wav files from  */dataset/transcription* (directory from the previous (#1-3) ASR labs). Take a subsample of 20 files, the same as you used in Exercise 1. Synthesize speech for these files several times using the multi-speaker SpeechBrain Tacotron 2 model and HiFi-GAN vocoder by providing different samples of different speakers.\n",
    "2. Listen to the obtained synthesised audio samples and evaluate them in terms of speech naturalness and intelligibility using the scale 1..5. You do not need to listen to all the synthesis files. For this subjective evaluation (listening test), only 1-2 samples of each speaker is sufficient to listen.\n",
    "3. Apply an objective metric (i.e. wv-MOS) to the synthesized files.\n",
    "4. Are there any differences in speech quality depending on the reference speaker voice? If yes, could you specify the most difficult/simple voice(s) for TTS synthesis? What are the characteristics of these voices and main differences?\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
